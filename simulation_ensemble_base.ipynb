{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import winsound\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.BaseAutoEncoder import BaseSeq2Seq\n",
    "from src.dataload.tabular import tabularDataset\n",
    "from src.utils import ensemble_inference, inference\n",
    "from src.simulation_trainer import BaseTrainer, NewTrainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def _normalize_tabular(df: pd.DataFrame, label_name: str = \"label\"):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df.drop(label_name, axis=1))\n",
    "    y = df[label_name]\n",
    "    return X, y\n",
    "\n",
    "def simul_split_train_valid_test(df: pd.DataFrame, train_ratio: float = 0.7):\n",
    "    X, y = _normalize_tabular(df)\n",
    "    tmp = pd.DataFrame(X)\n",
    "    tmp[\"label\"] = y\n",
    "    normal = tmp.loc[tmp[\"label\"] == 0, :].reset_index(drop=True)\n",
    "    abnormal = tmp.loc[tmp[\"label\"] == 1, :].reset_index(drop=True)\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "        normal.drop(\"label\", axis=1),\n",
    "        normal[\"label\"],\n",
    "        train_size=train_ratio,\n",
    "        random_state=42,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_val_test, y_val_test, train_size=0.5, random_state=42, shuffle=False\n",
    "    )\n",
    "    X_test[\"label\"] = y_test\n",
    "    X_test = pd.concat([X_test, abnormal]).reset_index(drop=True)\n",
    "    return (\n",
    "        X_train.values,\n",
    "        X_val.values,\n",
    "        X_test.drop(\"label\", axis=1).values,\n",
    "        y_train.values,\n",
    "        y_val.values,\n",
    "        X_test[\"label\"].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=512, data='tabular', device='cuda:0', early_stop_round=50, hidden_size=[2], initial_epochs=[5, 20], n_epochs=1000, project='my_paper', sampling_ratio=[0.01, 0.1], sampling_term=[1, 5], train_ratio=0.7, trainer_name='BaseTrainer', window_size=60)\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "config = {\n",
    "    'trainer_name' : 'BaseTrainer',\n",
    "    'project' : 'my_paper',\n",
    "    'train_ratio': .7,\n",
    "    'batch_size': 512,\n",
    "    'n_epochs': 1000,\n",
    "    'early_stop_round': 50,\n",
    "    'hidden_size': [2],\n",
    "    'window_size': 60,\n",
    "    'data': 'tabular',\n",
    "    # 'sampling_term': [1, 5],\n",
    "    'sampling_term': [1, 5],\n",
    "    # 'initial_epochs': [10],\n",
    "    'initial_epochs': [5, 20],\n",
    "    'sampling_ratio': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# gpu\n",
    "gpu_id = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "if gpu_id == 0:\n",
    "    config['device'] = 'cuda:0'\n",
    "else:\n",
    "    config['device'] = 'cpu'\n",
    "\n",
    "config = Namespace(**config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.deterministic = True\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "# torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 저장해야 할 것\n",
    "  - epoch=500일 때만\n",
    "    - top, down\n",
    "    - error\n",
    "  - 항상\n",
    "    - 모델 성능 result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시간이 너무 오래걸려서\n",
    "- big은 제외하고 일단 진행 -> 사이즈를 줄이던지 교수님들한테 피드백받고 추가하자\n",
    "- small에서 hidden_size=4의 경우는 데이터가 부족한 것 같다. 일단은 **h=4는 제외**하고 진행\n",
    "- **weight=0.5는 제외**하고 일단 진행\n",
    "- 500 epoch 부터는 sampling_term=1만 진행\n",
    "- simulation은 일단 500빼자 -> bench만 해보자\n",
    "- *BUGFIX* : sampling_term은 다 다시해야함\n",
    "### 일지 (1회당 30분 걸림, 일단 3회 진행)\n",
    "- `smallNormal.csv`\n",
    "    - epoch=1000 & early_stop=50: 3회\n",
    "    - epoch=500: 3회\n",
    "- `smallTestNoise_010_01.csv`\n",
    "    - epoch=1000 & early_stop=50: 3회\n",
    "    - epoch=500: 3회\n",
    "- `smallTestNoise_010_09.csv`\n",
    "    - epoch=1000 & early_stop=50: 3회\n",
    "- `smallTestNoise_010_05.csv`\n",
    "  - 제외\n",
    "- `smallTrainNoise_001_01.csv`\n",
    "  - epoch=1000 & early_stop=50: 3회\n",
    "- `smallTrainNoise_001_05.csv`\n",
    "  - 제외\n",
    "- `smallTrainNoise_001_09.csv`\n",
    "  - epoch=1000 & early_stop=50: 3회\n",
    "  - epoch=500 & init=20 & samTerm=[1,5,10]-> 1회 top down 분석\n",
    "- `smallTrainNoise_010_01.csv`\n",
    "  - epoch=1000 & early_stop=50: 3회\n",
    "  - epoch=500 & init=10 3회-> top down 분석\n",
    "- `smallTrainNoise_010_05.csv`\n",
    "  - 제외\n",
    "- `smallTrainNoise_010_09.csv`\n",
    "  - epoch=1000 & early_stop=50: 3회\n",
    "  - epoch=500 & init=10 1회-> top down 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "- 위의 config -> 총 8개 모델 이용해서 Ensemble\n",
    "\n",
    "```\n",
    "'sampling_term': [1, 5],\n",
    "'initial_epochs': [5, 20],\n",
    "'sampling_ratio': [0.01, 0.1]\n",
    "```\n",
    "\n",
    "- 실험진행 데이터\n",
    "  - `smallTrainNoise_010_09.csv`: 3 \n",
    "  - `smallTrainNoise_010_01.csv`: 3\n",
    "  - `smallTrainNoise_001_09.csv`: 3\n",
    "  - `smallTrainNoise_001_01.csv`: 3\n",
    "  - `smallTestNoise_010_09.csv`: 3\n",
    "  - `smallTestNoise_010_01.csv`: 3\n",
    "  - `smallNormal.csv`: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped! in Epoch 576:\n",
      "train_loss=0.591, valid_loss=0.593\n",
      "<< 2번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped! in Epoch 660:\n",
      "train_loss=0.584, valid_loss=0.585\n",
      "<< 3번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped! in Epoch 733:\n",
      "train_loss=0.582, valid_loss=0.586\n",
      "<< 4번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped! in Epoch 460:\n",
      "train_loss=0.638, valid_loss=0.637\n"
     ]
    }
   ],
   "source": [
    "data_list = ['smallTrainNoise_010_09.csv']\n",
    "\n",
    "n_time = 4\n",
    "PATH = './run_result_sim_nodrop/'\n",
    "\n",
    "for d in data_list:\n",
    "    data = pd.read_csv('./sim_data/' + d)\n",
    "    config.data_name = d.split('.')[0]\n",
    "        # 500epoch이고 train에 noise가 있는 경우에만 error, top, down 확인\n",
    "    if (config.n_epochs == 500) and ('Train' in config.data_name):\n",
    "        is_debug = True\n",
    "    else:\n",
    "        is_debug = False\n",
    "        \n",
    "    for i in range(n_time):\n",
    "\n",
    "        print(f'<< {i+1}번재 시작 >>')\n",
    "        (\n",
    "            train_x,\n",
    "            valid_x,\n",
    "            test_x,\n",
    "            train_y,\n",
    "            valid_y,\n",
    "            test_y,\n",
    "        ) = simul_split_train_valid_test(data, config.train_ratio)\n",
    "        \n",
    "        # resize 'window_size' = 'col_len'\n",
    "        config.window_size = train_x.shape[1]\n",
    "\n",
    "        train_dataset = tabularDataset(train_x, train_y)\n",
    "        valid_dataset = tabularDataset(valid_x, valid_y)\n",
    "        test_dataset = tabularDataset(test_x, test_y)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "\n",
    "        total_x = np.concatenate([train_x, valid_x, test_x])\n",
    "        total_y = np.concatenate([train_y, valid_y, test_y])\n",
    "        IR = round((len(total_y) - np.sum(total_y)) / np.sum(total_y), 4)\n",
    "        \n",
    "        # for inference\n",
    "        total_dataset = tabularDataset(total_x, total_y)\n",
    "        total_dataloader = DataLoader(\n",
    "            total_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "\n",
    "        if is_debug is False:\n",
    "            for hidden_size in config.hidden_size:\n",
    "                print(f\"-----BaseTrainer starts with hidden_size={hidden_size}-----\")\n",
    "                config.trainer_name = \"BaseTrainer\"\n",
    "\n",
    "                model = BaseSeq2Seq(\n",
    "                    input_size=config.window_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=config.window_size,\n",
    "                    dropout_p=0.0,\n",
    "                ).to(config.device)\n",
    "\n",
    "                optimizer = optim.Adam(model.parameters())\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                # train\n",
    "                trainer = BaseTrainer(model=model, optimizer=optimizer, crit=criterion)\n",
    "\n",
    "                train_loss, val_loss, return_epoch, best_model = trainer.train(\n",
    "                    train_loader=train_dataloader,\n",
    "                    val_loader=valid_dataloader,\n",
    "                    config=config,\n",
    "                    use_wandb=False,\n",
    "                )\n",
    "\n",
    "                best_model.to(\"cpu\")                        \n",
    "                sampling_term = 0\n",
    "                sampling_ratio = 0\n",
    "                initial_epoch = 0\n",
    "\n",
    "                df, tst_ano_score = ensemble_inference(\n",
    "                    config,\n",
    "                    total_dataloader,\n",
    "                    best_model,\n",
    "                    train_x,\n",
    "                    valid_x,\n",
    "                    total_x,\n",
    "                    total_y,\n",
    "                    return_epoch,\n",
    "                    hidden_size,\n",
    "                    train_loss,\n",
    "                    val_loss,\n",
    "                    IR,\n",
    "                    sampling_term,\n",
    "                    sampling_ratio,\n",
    "                    initial_epoch,\n",
    "                    PATH\n",
    "                )\n",
    "                df.to_csv(PATH + \"result_\" + config.data_name + \".csv\", index=False)\n",
    "                \n",
    "                #hp = '_hs' + str(hidden_size) + '_st' + str(sampling_term) + '_sr' + str(sampling_ratio) + '_ie' + str(initial_epoch)\n",
    "                with open('./ensemble_sim_base_2/' + config.data_name + str(i) + '.pickle', 'wb') as f:\n",
    "                    pickle.dump(tst_ano_score, f, pickle.HIGHEST_PROTOCOL)\n",
    "                torch.cuda.empty_cache()\n",
    "                                                \n",
    "frequency = 800\n",
    "duration = 2000\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">roc_auc</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pr_auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trainer_name</th>\n",
       "      <th>sampling_term</th>\n",
       "      <th>sampling_ratio</th>\n",
       "      <th>initial_epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BaseTrainer</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.726008</td>\n",
       "      <td>0.049049</td>\n",
       "      <td>0.665075</td>\n",
       "      <td>0.056311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          roc_auc            \\\n",
       "                                                             mean       std   \n",
       "trainer_name sampling_term sampling_ratio initial_epoch                       \n",
       "BaseTrainer  0             0              0              0.726008  0.049049   \n",
       "\n",
       "                                                           pr_auc            \n",
       "                                                             mean       std  \n",
       "trainer_name sampling_term sampling_ratio initial_epoch                      \n",
       "BaseTrainer  0             0              0              0.665075  0.056311  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./run_result_sim_base/result_' + config.data_name + '.csv')\n",
    "cols = ['trainer_name', 'sampling_term','sampling_ratio','initial_epoch']\n",
    "df.groupby(cols)[['roc_auc','pr_auc']].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.755279</td>\n",
       "      <td>0.74738</td>\n",
       "      <td>0.706091</td>\n",
       "      <td>0.798706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_auc</th>\n",
       "      <td>0.708022</td>\n",
       "      <td>0.696569</td>\n",
       "      <td>0.637348</td>\n",
       "      <td>0.755849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean    median       max       min\n",
       "roc_auc  0.755279   0.74738  0.706091  0.798706\n",
       "pr_auc   0.708022  0.696569  0.637348  0.755849"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "anomaly_score = np.zeros((len(test_y), 8))\n",
    "\n",
    "result_df = pd.DataFrame(columns=['mean','median','max','min'], index=['roc_auc','pr_auc'])\n",
    "\n",
    "PATH = './ensemble_sim_base_1/'\n",
    "idx = 0\n",
    "for data_name in os.listdir(PATH):\n",
    "    if config.data_name in data_name:\n",
    "        with open(PATH + data_name, 'rb') as f:\n",
    "            tmp = pickle.load(f)\n",
    "            anomaly_score[:, idx] = tmp\n",
    "            idx += 1\n",
    "\n",
    "tst_ano_scr_med = np.median(anomaly_score, axis=1)\n",
    "tst_ano_scr_mean = np.mean(anomaly_score, axis=1)\n",
    "tst_ano_scr_max = np.max(anomaly_score, axis=1)\n",
    "tst_ano_scr_min = np.min(anomaly_score, axis=1)\n",
    "\n",
    "idx = 0\n",
    "for tst_ano_scr in [tst_ano_scr_mean, tst_ano_scr_med, tst_ano_scr_max, tst_ano_scr_min]:\n",
    "    roc_auc = roc_auc_score(test_y, tst_ano_scr)\n",
    "    _precision, _recall, _ = precision_recall_curve(test_y, tst_ano_scr)\n",
    "    pr_auc = auc(_recall, _precision)\n",
    "    result_df.iloc[0, idx] = roc_auc\n",
    "    result_df.iloc[1, idx] = pr_auc\n",
    "    idx += 1\n",
    "\n",
    "result_df\n",
    "# if config.data == \"tabular\":\n",
    "#     tst_y_true = total_y[-len(tst_ano_scr):]\n",
    "# else:\n",
    "    # tst_y_true = total_y[tst_start_idx : len(window_anomaly_score_result)] # 여기 time_series 계산\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4829241e38f97ce9549fd9dd142f37034ca40da852944784ae6a4626739d5593"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
