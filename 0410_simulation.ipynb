{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.BaseAutoEncoder import BaseSeq2Seq\n",
    "from src.dataload.tabular import tabularDataset\n",
    "from src.utils import inference, ensemble_inference\n",
    "from src.simulation_trainer import BaseTrainer, NewTrainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def _normalize_tabular(df: pd.DataFrame, label_name: str = \"label\"):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df.drop(label_name, axis=1))\n",
    "    y = df[label_name]\n",
    "    return X, y\n",
    "\n",
    "def simul_split_train_valid_test(df: pd.DataFrame, train_ratio: float = 0.7):\n",
    "    X, y = _normalize_tabular(df)\n",
    "    tmp = pd.DataFrame(X)\n",
    "    tmp[\"label\"] = y\n",
    "    normal = tmp.loc[tmp[\"label\"] == 0, :].reset_index(drop=True)\n",
    "    abnormal = tmp.loc[tmp[\"label\"] == 1, :].reset_index(drop=True)\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "        normal.drop(\"label\", axis=1),\n",
    "        normal[\"label\"],\n",
    "        train_size=train_ratio,\n",
    "        random_state=42,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_val_test, y_val_test, train_size=0.5, random_state=42, shuffle=False\n",
    "    )\n",
    "    X_test[\"label\"] = y_test\n",
    "    X_test = pd.concat([X_test, abnormal]).reset_index(drop=True)\n",
    "    return (\n",
    "        X_train.values,\n",
    "        X_val.values,\n",
    "        X_test.drop(\"label\", axis=1).values,\n",
    "        y_train.values,\n",
    "        y_val.values,\n",
    "        X_test[\"label\"].values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=512, data='tabular', device='cuda:0', early_stop_round=1000, hidden_size=[2], initial_epochs=[5, 20], n_epochs=500, project='my_paper', sampling_ratio=[0.01, 0.1], sampling_term=[1, 5], train_ratio=0.7, trainer_name='BaseTrainer', window_size=60)\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "config = {\n",
    "    'trainer_name' : 'BaseTrainer',\n",
    "    'project' : 'my_paper',\n",
    "    'train_ratio': .7,\n",
    "    'batch_size': 512,\n",
    "    'n_epochs': 500,\n",
    "    'early_stop_round': 1000,\n",
    "    # 'hidden_size': [2, 4], # -> 더 큰 데이터에서 4를 진행해야 할 듯 (나중에 benchmark?)\n",
    "    'hidden_size': [2],\n",
    "    'window_size': 60,\n",
    "    'data': 'tabular',\n",
    "    # 'sampling_term': [1, 5],\n",
    "    'sampling_term': [1, 5],\n",
    "    # 'initial_epochs': [10],\n",
    "    'initial_epochs': [5, 20],\n",
    "    'sampling_ratio': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# gpu\n",
    "gpu_id = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "if gpu_id == 0:\n",
    "    config['device'] = 'cuda:0'\n",
    "else:\n",
    "    config['device'] = 'cpu'\n",
    "\n",
    "config = Namespace(**config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.deterministic = True\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "# torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abalone9-18.csv',\n",
       " 'shuttle-c0-vs-c4.csv',\n",
       " 'vowel0.csv',\n",
       " 'wine.csv',\n",
       " 'yeast-1-2-8-9_vs_7.csv',\n",
       " 'yeast4.csv',\n",
       " 'yeast5.csv',\n",
       " 'yeast6.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PATH = './tabular_data'\n",
    "file_list = os.listdir(PATH)\n",
    "file_list_csv = [file for file in file_list if file.endswith('.csv')]\n",
    "file_list_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.029, valid_loss=0.027\n",
      "New file generated!\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.018, valid_loss=0.009\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.166, valid_loss=0.005\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.052, valid_loss=0.014\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.008, valid_loss=0.007\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.031, valid_loss=0.019\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.010, valid_loss=0.012\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.057, valid_loss=0.025\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.071, valid_loss=0.087\n",
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.521, valid_loss=0.636\n",
      "New file generated!\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.500, valid_loss=0.531\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.442, valid_loss=0.566\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.471, valid_loss=0.500\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.534, valid_loss=0.591\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.416, valid_loss=0.495\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.483, valid_loss=0.602\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.413, valid_loss=0.515\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.495, valid_loss=0.668\n",
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.448, valid_loss=0.495\n",
      "New file generated!\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.480, valid_loss=0.507\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.468, valid_loss=0.504\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.444, valid_loss=0.479\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.448, valid_loss=0.504\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.434, valid_loss=0.530\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.411, valid_loss=0.480\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.421, valid_loss=0.489\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.406, valid_loss=0.512\n",
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.323, valid_loss=0.715\n",
      "New file generated!\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.327, valid_loss=0.534\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.396, valid_loss=0.617\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.274, valid_loss=0.462\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.386, valid_loss=0.699\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.297, valid_loss=0.588\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.423, valid_loss=0.744\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.346, valid_loss=0.698\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.350, valid_loss=0.569\n",
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.647, valid_loss=0.509\n",
      "New file generated!\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.427, valid_loss=0.425\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.503, valid_loss=0.482\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.495, valid_loss=0.438\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.493, valid_loss=0.506\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.500, valid_loss=0.572\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.474, valid_loss=0.479\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.363, valid_loss=0.408\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.369, valid_loss=0.425\n",
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.568, valid_loss=0.539\n",
      "New file generated!\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.630, valid_loss=0.662\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.333, valid_loss=0.449\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.504, valid_loss=0.506\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.469, valid_loss=0.512\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.495, valid_loss=0.627\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.313, valid_loss=0.467\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.325, valid_loss=0.493\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.275, valid_loss=0.407\n",
      "<< 1번재 시작 >>\n",
      "-----BaseTrainer starts with hidden_size=2-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c9eb5218e970>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"label\"] = y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.374, valid_loss=0.463\n",
      "New file generated!\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.572, valid_loss=0.689\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.409, valid_loss=0.512\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.369, valid_loss=0.478\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.391, valid_loss=0.385\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.373, valid_loss=0.518\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.375, valid_loss=0.561\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.399, valid_loss=0.564\n",
      "-----NewTrainer starts-----\n",
      "train_loss=0.294, valid_loss=0.408\n"
     ]
    }
   ],
   "source": [
    "data_list = file_list[1:]\n",
    "\n",
    "n_time = 1\n",
    "save_idx = '_' + str(0)\n",
    "# 모델 성능 결과 저장 경로\n",
    "PATH = './0410_run_result_tabular/'\n",
    "ENSEMBLE_PATH = './0410_ensemble_sim_1/'\n",
    "\n",
    "for d in data_list:\n",
    "    data = pd.read_csv('./tabular_data/' + d)\n",
    "    config.data_name = d.split('.')[0]\n",
    "    #     # 500epoch이고 train에 noise가 있는 경우에만 error, top, down 확인\n",
    "    # if (config.n_epochs == 500) and ('Train' in config.data_name):\n",
    "    #     is_debug = True\n",
    "    # else:\n",
    "    #     is_debug = False\n",
    "    is_debug = False\n",
    "        \n",
    "    for i in range(n_time):\n",
    "\n",
    "        print(f'<< {i+1}번재 시작 >>')\n",
    "        (\n",
    "            train_x,\n",
    "            valid_x,\n",
    "            test_x,\n",
    "            train_y,\n",
    "            valid_y,\n",
    "            test_y,\n",
    "        ) = simul_split_train_valid_test(data, config.train_ratio)\n",
    "        \n",
    "        # resize 'window_size' = 'col_len'\n",
    "        config.window_size = train_x.shape[1]\n",
    "\n",
    "        train_dataset = tabularDataset(train_x, train_y)\n",
    "        valid_dataset = tabularDataset(valid_x, valid_y)\n",
    "        test_dataset = tabularDataset(test_x, test_y)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "\n",
    "        total_x = np.concatenate([train_x, valid_x, test_x])\n",
    "        total_y = np.concatenate([train_y, valid_y, test_y])\n",
    "        IR = round((len(total_y) - np.sum(total_y)) / np.sum(total_y), 4)\n",
    "        \n",
    "        # for inference\n",
    "        total_dataset = tabularDataset(total_x, total_y)\n",
    "        total_dataloader = DataLoader(\n",
    "            total_dataset, shuffle=False, batch_size=config.batch_size\n",
    "        )\n",
    "\n",
    "        if is_debug is False:\n",
    "            for hidden_size in config.hidden_size:\n",
    "                print(f\"-----BaseTrainer starts with hidden_size={hidden_size}-----\")\n",
    "                config.trainer_name = \"BaseTrainer\"\n",
    "\n",
    "                model = BaseSeq2Seq(\n",
    "                    input_size=config.window_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=config.window_size,\n",
    "                    dropout_p=0.0,\n",
    "                ).to(config.device)\n",
    "\n",
    "                optimizer = optim.Adam(model.parameters())\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                # train\n",
    "                trainer = BaseTrainer(model=model, optimizer=optimizer, crit=criterion)\n",
    "\n",
    "                train_loss, val_loss, return_epoch, best_model = trainer.train(\n",
    "                    train_loader=train_dataloader,\n",
    "                    val_loader=valid_dataloader,\n",
    "                    config=config,\n",
    "                    use_wandb=False,\n",
    "                )\n",
    "\n",
    "                best_model.to(\"cpu\")                        \n",
    "                sampling_term = 0\n",
    "                sampling_ratio = 0\n",
    "                initial_epoch = 0\n",
    "\n",
    "                df, tst_ano_score = ensemble_inference(\n",
    "                    config,\n",
    "                    total_dataloader,\n",
    "                    best_model,\n",
    "                    train_x,\n",
    "                    valid_x,\n",
    "                    total_x,\n",
    "                    total_y,\n",
    "                    return_epoch,\n",
    "                    hidden_size,\n",
    "                    train_loss,\n",
    "                    val_loss,\n",
    "                    IR,\n",
    "                    sampling_term,\n",
    "                    sampling_ratio,\n",
    "                    initial_epoch,\n",
    "                    PATH\n",
    "                )\n",
    "                \n",
    "                df.to_csv(PATH + \"result_\" + config.data_name + \".csv\", index=False)\n",
    "                \n",
    "                hp = '_hs' + str(hidden_size) + '_st' + str(sampling_term) + '_sr' + str(sampling_ratio) + '_ie' + str(initial_epoch)\n",
    "                with open(ENSEMBLE_PATH + 'base_' + config.data_name + hp + '.pickle', 'wb') as f:\n",
    "                    pickle.dump(tst_ano_score, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        for hidden_size in config.hidden_size:\n",
    "            for sampling_ratio in config.sampling_ratio:\n",
    "                for initial_epoch in config.initial_epochs: \n",
    "                    for sampling_term in config.sampling_term:\n",
    "                        print(\n",
    "                            f\"-----NewTrainer starts-----\"\n",
    "                        )\n",
    "                        config.trainer_name = \"NewTrainer\"\n",
    "\n",
    "                        model = BaseSeq2Seq(\n",
    "                            input_size=config.window_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            output_size=config.window_size,\n",
    "                            dropout_p=0.0,\n",
    "                        ).to(config.device)\n",
    "\n",
    "                        optimizer = optim.Adam(model.parameters())\n",
    "                        criterion = nn.MSELoss()\n",
    "\n",
    "                        # train\n",
    "                        trainer = NewTrainer(model=model, optimizer=optimizer, crit=criterion)\n",
    "                        \n",
    "                        train_loss, val_loss, return_epoch, best_model, errors, tops, downs = trainer.train(\n",
    "                            train_x=train_x,\n",
    "                            train_y=train_y,\n",
    "                            train_loader=train_dataloader,\n",
    "                            val_loader=valid_dataloader,\n",
    "                            sampling_term=sampling_term,\n",
    "                            initial_epoch=initial_epoch,\n",
    "                            sampling_ratio=sampling_ratio,\n",
    "                            config=config,\n",
    "                            use_wandb=False,\n",
    "                            is_debug=is_debug\n",
    "                        )\n",
    "                        if is_debug:\n",
    "                            hp = '_h' + str(hidden_size) + '_st' + str(sampling_term) + '_sr' + str(sampling_ratio) + '_ie' + str(initial_epoch)\n",
    "                            with open('./run_result_sim_error/newError_'+ config.data_name + hp + save_idx + '.pickle', 'wb') as f:\n",
    "                                pickle.dump(errors, f, pickle.HIGHEST_PROTOCOL)\n",
    "                            with open('./run_result_sim_top/newTop_'+ config.data_name + hp + save_idx + '.pickle', 'wb') as f:\n",
    "                                pickle.dump(tops, f, pickle.HIGHEST_PROTOCOL)\n",
    "                            with open('./run_result_sim_down/newDown_'+ config.data_name + hp + save_idx + '.pickle', 'wb') as f:\n",
    "                                pickle.dump(downs, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        \n",
    "                        best_model.to(\"cpu\")                                \n",
    "                        \n",
    "                        df, tst_ano_score = ensemble_inference(\n",
    "                            config,\n",
    "                            total_dataloader,\n",
    "                            best_model,\n",
    "                            train_x,\n",
    "                            valid_x,\n",
    "                            total_x,\n",
    "                            total_y,\n",
    "                            return_epoch,\n",
    "                            hidden_size,\n",
    "                            train_loss,\n",
    "                            val_loss,\n",
    "                            IR,\n",
    "                            sampling_term,\n",
    "                            sampling_ratio,\n",
    "                            initial_epoch,\n",
    "                            PATH\n",
    "                        )\n",
    "                        \n",
    "                        df.to_csv(PATH + \"result_\" + config.data_name + \".csv\", index=False)\n",
    "                        \n",
    "                        hp = '_hs' + str(hidden_size) + '_st' + str(sampling_term) + '_sr' + str(sampling_ratio) + '_ie' + str(initial_epoch)\n",
    "                        with open(ENSEMBLE_PATH + 'new_' + config.data_name + hp + '.pickle', 'wb') as f:\n",
    "                            pickle.dump(tst_ano_score, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        \n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "frequency = 800\n",
    "duration = 2000\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['result_abalone9-18.csv',\n",
       " 'result_shuttle-c0-vs-c4.csv',\n",
       " 'result_vowel0.csv',\n",
       " 'result_wine.csv',\n",
       " 'result_yeast-1-2-8-9_vs_7.csv',\n",
       " 'result_yeast4.csv',\n",
       " 'result_yeast5.csv',\n",
       " 'result_yeast6.csv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./0410_run_result_tabular') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">roc_auc</th>\n",
       "      <th colspan=\"2\" halign=\"left\">pr_auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trainer_name</th>\n",
       "      <th>sampling_term</th>\n",
       "      <th>sampling_ratio</th>\n",
       "      <th>initial_epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BaseTrainer</th>\n",
       "      <th>0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0</th>\n",
       "      <td>0.6241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1941</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">NewTrainer</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.01</th>\n",
       "      <th>5</th>\n",
       "      <td>0.7567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2808</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.10</th>\n",
       "      <th>5</th>\n",
       "      <td>0.6569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.7350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.01</th>\n",
       "      <th>5</th>\n",
       "      <td>0.7465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2826</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.10</th>\n",
       "      <th>5</th>\n",
       "      <td>0.5623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        roc_auc      pr_auc  \\\n",
       "                                                           mean std    mean   \n",
       "trainer_name sampling_term sampling_ratio initial_epoch                       \n",
       "BaseTrainer  0             0.00           0              0.6241 NaN  0.1941   \n",
       "NewTrainer   1             0.01           5              0.7567 NaN  0.2808   \n",
       "                                          20             0.6950 NaN  0.2647   \n",
       "                           0.10           5              0.6569 NaN  0.2210   \n",
       "                                          20             0.7350 NaN  0.2977   \n",
       "             5             0.01           5              0.7465 NaN  0.2826   \n",
       "                                          20             0.6578 NaN  0.2195   \n",
       "                           0.10           5              0.5623 NaN  0.1452   \n",
       "                                          20             0.6388 NaN  0.2125   \n",
       "\n",
       "                                                             \n",
       "                                                        std  \n",
       "trainer_name sampling_term sampling_ratio initial_epoch      \n",
       "BaseTrainer  0             0.00           0             NaN  \n",
       "NewTrainer   1             0.01           5             NaN  \n",
       "                                          20            NaN  \n",
       "                           0.10           5             NaN  \n",
       "                                          20            NaN  \n",
       "             5             0.01           5             NaN  \n",
       "                                          20            NaN  \n",
       "                           0.10           5             NaN  \n",
       "                                          20            NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['trainer_name', 'sampling_term','sampling_ratio','initial_epoch']\n",
    "name = os.listdir('./0410_run_result_tabular')[7]\n",
    "tmp = pd.read_csv('./0410_run_result_tabular/' + name)\n",
    "tmp.groupby(cols)[['roc_auc','pr_auc']].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# anomaly_score = np.zeros((len(test_y), 8))\n",
    "\n",
    "# result_df = pd.DataFrame(columns=['mean','median','max','min'], index=['roc_auc','pr_auc'])\n",
    "\n",
    "# idx = 0\n",
    "# for data_name in os.listdir(ENSEMBLE_PATH):\n",
    "#     if config.data_name in data_name and 'base' in data_name:\n",
    "#         with open(ENSEMBLE_PATH  + data_name, 'rb') as f:\n",
    "#             tmp = pickle.load(f)\n",
    "#             anomaly_score[:, idx] = tmp\n",
    "#             idx += 1\n",
    "\n",
    "# tst_ano_scr_med = np.median(anomaly_score, axis=1)\n",
    "# tst_ano_scr_mean = np.mean(anomaly_score, axis=1)\n",
    "# tst_ano_scr_max = np.max(anomaly_score, axis=1)\n",
    "# tst_ano_scr_min = np.min(anomaly_score, axis=1)\n",
    "\n",
    "# idx = 0\n",
    "# for tst_ano_scr in [tst_ano_scr_mean, tst_ano_scr_med, tst_ano_scr_max, tst_ano_scr_min]:\n",
    "#     roc_auc = roc_auc_score(test_y, tst_ano_scr)\n",
    "#     _precision, _recall, _ = precision_recall_curve(test_y, tst_ano_scr)\n",
    "#     pr_auc = auc(_recall, _precision)\n",
    "#     result_df.iloc[0, idx] = roc_auc\n",
    "#     result_df.iloc[1, idx] = pr_auc\n",
    "#     idx += 1\n",
    "\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.694233</td>\n",
       "      <td>0.671298</td>\n",
       "      <td>0.733159</td>\n",
       "      <td>0.704587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_auc</th>\n",
       "      <td>0.248507</td>\n",
       "      <td>0.233635</td>\n",
       "      <td>0.255832</td>\n",
       "      <td>0.289785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean    median       max       min\n",
       "roc_auc  0.694233  0.671298  0.733159  0.704587\n",
       "pr_auc   0.248507  0.233635  0.255832  0.289785"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "anomaly_score = np.zeros((len(test_y), 8))\n",
    "\n",
    "result_df = pd.DataFrame(columns=['mean','median','max','min'], index=['roc_auc','pr_auc'])\n",
    "\n",
    "idx = 0\n",
    "for data_name in os.listdir(ENSEMBLE_PATH):\n",
    "    if config.data_name in data_name and 'new' in data_name:\n",
    "        with open(ENSEMBLE_PATH  + data_name, 'rb') as f:\n",
    "            tmp = pickle.load(f)\n",
    "            anomaly_score[:, idx] = tmp\n",
    "            idx += 1\n",
    "\n",
    "tst_ano_scr_med = np.median(anomaly_score, axis=1)\n",
    "tst_ano_scr_mean = np.mean(anomaly_score, axis=1)\n",
    "tst_ano_scr_max = np.max(anomaly_score, axis=1)\n",
    "tst_ano_scr_min = np.min(anomaly_score, axis=1)\n",
    "\n",
    "idx = 0\n",
    "for tst_ano_scr in [tst_ano_scr_mean, tst_ano_scr_med, tst_ano_scr_max, tst_ano_scr_min]:\n",
    "    roc_auc = roc_auc_score(test_y, tst_ano_scr)\n",
    "    _precision, _recall, _ = precision_recall_curve(test_y, tst_ano_scr)\n",
    "    pr_auc = auc(_recall, _precision)\n",
    "    result_df.iloc[0, idx] = roc_auc\n",
    "    result_df.iloc[1, idx] = pr_auc\n",
    "    idx += 1\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4829241e38f97ce9549fd9dd142f37034ca40da852944784ae6a4626739d5593"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
